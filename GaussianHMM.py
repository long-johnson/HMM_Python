# -*- coding: utf-8 -*-

import numpy as np
import scipy as sp
from scipy import stats
import copy
import StandardImputationMethods as imp

class GHMM:
    """Implementation of Hidden Markov Model where observation density is
    represented by a mixture of normal distributions  
    Observations are vectors of real numbers
       
       Attributes
        ----------
        N : integer
            number of hidden states
        M : integer 
            number of distribution mixture components
        Z : integer 
            dimension of observations
        pi : integer 1darray (N)
            initial state distribution vector
        a : 2darray (NxN)
            transition probabilities matrix 
        tau : 2darray (NxM)
            weights of mixture distributions 
        mu : 3darray (NxMxZ)
            means of normal distributions
        sig : 4darray (NxMxZ)
            covariation matrix of normal distributions
    """
    
    def __init__(self, n, m, z, mu, sig, pi=None, a=None, tau=None, seed=None):
        """ 
        
        Parameters
        ----------
        n : integer
            number of hidden states
        m : integer 
            number of distribution mixture components
        z : integer 
            dimension of observations
        pi : integer 1darray (N)
            initial state distribution vector
        a : 2darray (NxN)
            transition probabilities matrix 
        tau : 2darray (NxM)
            weights of mixture distributions 
        mu : 3darray (NxMxZ)
            means of normal distributions
        sig : 4darray (NxMxZ)
            covariation matrix of normal distributions
        """
        if seed is not None:
            np.random.seed(seed)  
        self._n = n
        self._m = m
        self._z = z
        # if parameters are not defined - give them some statistically correct
        # default values or generated randomly if seed is provided
        if pi is not None:
            self._pi = np.array(pi)
        elif seed is None:
            self._pi = np.full(n, 1.0/n)
        else:
            self._pi = _generate_discrete_distribution(n)
        # transition matrix
        if a is not None:
            self._a = np.array(a)
        elif seed is None:
            self._a = np.full((n, n), 1.0/n)
        else:
            self._a = np.empty(shape=(n, n))
            for i in range(n):
                self._a[i, :] = _generate_discrete_distribution(n)      
        # mixture weights
        if tau is not None:
            self._tau = np.array(tau)
        elif seed is None:
            self._tau = np.full((n,m), 1.0/m)
        else:
            self._tau = np.empty(shape=(n, m))
            for i in range(n):
                self._tau[i,:] = _generate_discrete_distribution(m)
        # TODO: add random generation of mu and sig
        self._mu = np.array(mu)
        self._sig = np.array(sig)
        
    def generate_sequences(self, K, T, seed=None):
        """
        Generate sequences of observations produced by this model
        
        Parameters
        ----------
        K : int
            number of sequences
        T : int
            Length of each sequence
        seed : int, optional
            Seed for random generator
            
        Returns
        -------
        seqs : list of 1darrays
            List of generated sequences
        state_seqs : list of 1darrays
            List of hidden states sequences used for generation
        """
        # preparation
        # randomize with accordance to seed
        if seed is not None:
            np.random.seed(seed) 
        # prepare list for sequences
        seqs = [np.empty((T,self._z),dtype=np.float64) for k in range(K)]
        state_seqs = [np.empty(T, dtype=np.int32) for k in range(K)]
        # generation
        for k in range(K):
            state = _get_sample_discrete_distr(self._pi)
            state_seqs[k][0] = state
            mix_elem = _get_sample_discrete_distr(self._tau[state,:])
            cov_matrix = self._sig[state, mix_elem]
            seqs[k][0] = \
                np.random.multivariate_normal(self._mu[state, mix_elem],
                                              cov_matrix)
            for t in range(1, T):
                state = _get_sample_discrete_distr(self._a[state,:])
                state_seqs[k][t] = state
                mix_elem = _get_sample_discrete_distr(self._tau[state,:])
                cov_matrix = self._sig[state, mix_elem]
                seqs[k][t] = \
                    np.random.multivariate_normal(self._mu[state, mix_elem],
                                                  cov_matrix)
        return seqs, state_seqs
    
    def calc_likelihood(self, seqs, avails=None):
        """
        Calc likelihood of the sequences being generated by the current HMM
        
        Parameters
        ----------
        seqs : list of 2darrays (T x Z)
            observations sequences
        avails : list of boolean 1darrays (T), optional
            indicate whether element of sequence is not missing,
            i.e. True - not missing, False - is missing
            
        Returns
        -------
        likelihood : float64
            likelihood of the sequences being generated by the current HMM
        """
        likelihood = 0.0
        for k in range(len(seqs)):
            seq = seqs[k]
            avail = avails[k] if avails is not None \
                              else np.full(seq.shape[0], True, dtype=np.bool)
            b, _ = self._calc_b(seq, avail)
            likelihood += self._calc_forward_scaled(b)[0]
        return likelihood
   
    def _calc_b(self, seq, avail):
        """
        Calc conditional densities of each sequence element given each HMM state
        
        Parameters
        ----------
        seq : 2darray (T x Z)
            observations sequence 
        avail : boolean 1darray (T)
            indicate whether element of sequence is not missing,
            i.e. True - not missing, False - is missing
        
        Returns
        -------
        b : 2darray (T x N)
            conditional densities for each sequence element and HMM state, i.e.
            probabilities of generating element given that hmm was in the specific state
        g : 3darray (T x N x M)
            pdf (Gaussian distribution) values for each sequence element, given
            specific hidden state and specific mixture element
        """
        N = self._n
        M = self._m
        T = seq.shape[0]
        mu = self._mu
        sig = self._sig
        tau = self._tau
        g = np.empty((T, N, M))
        for t in range(T):
            if avail[t]:
                for i in range(N):
                    for m in range(M):
                        temp = sp.stats.multivariate_normal.pdf(seq[t], mu[i,m], sig[i,m],
                                                                allow_singular=True)
                        if temp == 0.0:
                            temp = 1.0e-200 # to prevent underflow
                        g[t, i, m] = temp
            else:
                g[t,:,:] = 1.0 # TODO: decide: nan or 1.0 ?
        b = np.sum(tau * g, axis=2)
        # set Bs that correspond to missing values to 1.0
        #b[np.logical_not(avail),:] = 1.0
        return b, g
    
    def _calc_forward_scaled(self, b):
        """
        Calc scaled forward variables
        
        Parameters
        ----------
        b : 2darray (T x N)
            conditional densities for each sequence element and HMM state
        
        Returns
        ----------
        likelihood : float64
            likelihood of the sequence being generated by the current HMM
        alpha : 2darray (T x N)
            scaled forward variables
        c : 2darray (T)
            scale coefficients
        """
        N = self._n
        T = b.shape[0]       
        pi = self._pi
        # memory
        alpha = np.empty(shape=(T, N))
        c = np.empty(T)
        # initialization
        alpha_t = pi * b[0]
        c[0] = 1.0 / np.sum(alpha_t)
        alpha[0,:] = c[0] * alpha_t 
        # induction
        a_T = np.transpose(self._a)
        for t in range(T-1):
            alpha_t = b[t+1,:] * np.sum(alpha[t,:]*a_T, axis=1)
            c[t+1] = 1.0 / np.sum(alpha_t)
            alpha[t+1,:] = c[t+1] * alpha_t
        # termination:
        loglikelihood = -np.sum(np.log(c))
        return loglikelihood, alpha, c
    
    def _calc_backward_scaled(self, b, c):
        """
        Calc scaled backward variables
        
        Parameters
        ----------
        b : 2darray (T x N)
            conditional densities for each sequence element and HMM state
        c : 2darray (T)
            scaling coefficients
        
        Returns
        ----------
        beta : 2darray (T x N)
            scaled backward variables
        """
        N = self._n
        T = b.shape[0]       
        a = self._a
        # memory
        beta = np.empty(shape=(T, N))
        # initialization
        beta_t = np.full(N, 1.0)
        beta[-1,:] = c[-1] * beta_t
        # induction
        for t in reversed(range(T-1)):
            beta_t = np.sum(a * b[t+1,:]  * beta[t+1,:], axis=1)
            beta[t,:] = c[t] * beta_t
        return beta
        
    def _calc_xi_gamma_scaled(self, b, alpha, beta):
        """ Calc xi(t,i,j), t=1..T, i,j=1..N - array of probabilities of
        being in state i and go to state j in time t given the model and seq
        Calc gamma(t,i), t=1..T, i=1..N -- array of probabilities of
        being in state i at the time t given the model and sequence
        
        Parameters
        ----------
        b : 2darray (T x N)
            conditional densities for each sequence element and HMM state
        alpha : 2darray (TxN)
            forward variables
        beta : 2darray (TxN)
            backward variables
            
        Returns
        -------
        xi : 3darray (TxNxN)
            probs of transition from i to j at time t given the model and seq
        gamma : 2darray (TxN)
            probs of transition from i at time t given the model and seq
        """
        T = b.shape[0]
        N = self._n
        xi = np.empty(shape=(T-1, N, N))
        a_tr = np.transpose(self._a)
        for t in range(T-1):           
            xi[t,:,:] = (alpha[t,:] * a_tr).T * b[t+1,:] * beta[t+1,:]
        gamma = np.sum(xi, axis=2)
        return xi, gamma
     
    def _calc_gamma_m_scaled(self, b, g, gamma):
        """ Calc gamma_m(t,i,m), t=1..T, i=1..N, m=1..M -- array of probs
        of being in state i at time t and selecting m-th mixture component
        
        Parameters
        ----------
        b : 2darray (T x N)
            conditional densities for each sequence element and HMM state
        g : 3darray (T x N x M)
            pdf (Gaussian distribution) values for each sequence element
        gamma : 2darray (TxN)
            probs of transition from i at time t given the model and seq
            
        Returns
        -------
        gamma_m : 3darray (TxNxM)
            probs of transition from i at time t and selection of m-th mixture
        """
        N = self._n
        M = self._m
        T = b.shape[0]       
        tau = self._tau
        gamma_m = np.empty(shape=(T-1, N, M))
        gamma_m = g[:-1,:,:] * tau * gamma[:,:, np.newaxis] / b[:-1,:,np.newaxis]
        return gamma_m
    
    def train_baumwelch(self, seqs, rtol, max_iter, avails=None):
        """ Adjust the parameters of the HMM using Baum-Welch algorithm
        
        Parameters
        ----------
        seqs : list of float64 2darrays (TxZ)
            training sequences
            Note: len(seqs) = K
        rtol : float64
            relative tolerance (stopping criterion)
        max_iter : float64, optional
            maximum number of Baum-Welch iterations (stopping criterion)
        avails : list of boolean 1darrays (T), optional
            arrays that indicate whether each element of each sequence is 
            not missing (availiable), i.e. True - not missing, False - is missing
            Note: len(avails) = K
            
        Returns
        -------
        likelihood : float64
            total likelihood of training seqs being produced by the trained HMM
        iteration : int
            iteration reached during baum-welch training process
        """
        N = self._n
        M = self._m
        Z = self._z
        K = len(seqs)
        T = max([len(seq) for seq in seqs]) # for gamma_ms
        iteration = 0
        p_prev = -100.0 # likelihood on previous iteration
        p = 100.0       # likelihood on cur iteration
        while np.abs((p_prev-p)/p) > rtol and iteration < max_iter:
            p_prev = p
            # calculate numenator and denominator for re-estimation
            pi_up = np.zeros(N)
            a_up = np.zeros((N, N))
            tau_up = np.zeros((N, M))
            a_down = np.zeros(N)
            tau_down = np.zeros(N)
            mu_up = np.zeros((N, M, Z))
            mu_sig_down = np.zeros((N, M))
            gamma_ms = np.zeros((K,T-1,N,M))
            sig_up = np.zeros((N, M, Z, Z))
            for k in range(K):   
                # expectation
                seq = seqs[k]
                avail = avails[k] if avails is not None \
                                  else np.full(seq.shape[0], True, dtype=np.bool)
                b, g = self._calc_b(seq, avail)
                p, alpha, c = self._calc_forward_scaled(b)
                beta = self._calc_backward_scaled(b, c)
                xi, gamma = self._calc_xi_gamma_scaled(b, alpha, beta)
                gamma_ms[k] = self._calc_gamma_m_scaled(b, g, gamma)
                # accumulating for maximization
                temp_avail_last = avail[-1]  # not to touch the last element
                avail[-1] = False            # not to touch the last element
                pi_up += gamma[0,:]
                a_up += np.sum(xi, axis=0)
                a_down += np.sum(gamma, axis=0)                
                tau_up += np.sum(gamma_ms[k][avail], axis=0)
                tau_down += np.sum(gamma[avail], axis=0)  
                mu_up += np.einsum('tnm,tz->nmz', gamma_ms[k][avail], seq[avail])
                mu_sig_down += np.sum(gamma_ms[k][avail], axis=0)
                avail[-1] = temp_avail_last  # to restore original value
            # re-estimation
            self._pi = pi_up / K
            self._a = (a_up.T / a_down).T
            self._tau = (tau_up.T / tau_down).T
            self._mu = mu_up / mu_sig_down[:,:,np.newaxis]
            # accumulating sig
            # TODO: is it possible to optimize this ...?
            for k in range(K):
                seq = seqs[k]
                avail = avails[k] if avails is not None \
                                  else np.full(seq.shape[0], True, dtype=np.bool)
                T = seq.shape[0]
                for t in range(T-1):
                    if avail[t]:
                        diff = self._mu - seq[t]
                        for i in range(N):
                            for m in range(M):
                                sig_up[i,m] += gamma_ms[k,t,i,m] * \
                                               diff[i,m] * (diff[i,m]).reshape((Z,1))
                                           #instead of np.outer()
            # sig re-estimation
            self._sig = sig_up / mu_sig_down[:,:,np.newaxis,np.newaxis]
            iteration += 1
        likelihood = self.calc_likelihood(seqs, avails)
        return likelihood, iteration
        
    def decode_viterbi(self, seqs, avails):
        """ Infer the sequence of hidden states that were reached during generation
            multiple sequences version
        
        Parameters
        ----------
        seqs : list of float64 2darray
            observation sequence
        avail : boolean 1darray (T)
            indicate whether element of sequence is not missing,
            i.e. True - not missing, False - is missing
        
        Returns
        -------
        states_list : int 1darray
            Inferred sequence of hidden states    
        """
        K = len(seqs)
        states_list = []
        for k in range(K): 
            states_list.append(self._decode_viterbi(seqs[k], avails[k]))
        return states_list
        
    def _decode_viterbi(self, seq, avail):
        """ Infer the sequence of hidden states that were reached during generation
            single sequence version
        
        seq : float64 2darray
            observation sequence
        avail : boolean 1darray (T)
            indicate whether element of sequence is not missing,
            i.e. True - not missing, False - is missing
        
        Returns
        -------
        states : int 1darray
            Inferred sequence of hidden states                
        """
        T = seq.shape[0]
        N = self._n
        pi = self._pi
        log_a_tr = (np.log(self._a)).T
        b, _ = self._calc_b(seq, avail)
        log_b = np.log(b)
        psi = np.empty(shape=(T, N), dtype=np.int32)
        row_idx = np.arange(N) # to select max columns
        # initialization
        delta = np.log(pi) + log_b[0, :]     
        # recursion
        for t in range(1,T):
            temp = delta + log_a_tr
            argmax = np.argmax(temp, axis=1)
            psi[t,:] = argmax
            delta = temp[row_idx, argmax] + log_b[t, :]
        # backtracking
        q = np.empty(T, dtype=np.int32)
        q[-1] = np.argmax(delta)
        for t in reversed(range(T-1)):
            q[t] = psi[t+1, q[t+1]]
        return q
    
    def impute_by_states(self, seqs_, avails, states_list):
        """ Impute gaps according to the most probable hidden states path.
        Gap is imputed with mean that corresponds to the inferred hidden state
        and the most probable mixture component
        
        Parameters
        ----------
        seqs_ : list of float64 2darray
            observation sequences with gaps
        avails : list of boolean 1darrays (T)
            arrays that indicate whether element of sequence is not missing,
            i.e. True - not missing, False - is missing
        states_list : int 1darray
            Inferred sequence of hidden states
            
        Returns
        -------
        seqs : list of float64 2darray
            imputed observation sequences
        """
        seqs = copy.deepcopy(seqs_)
        K = len(seqs)
        M = self._m
        mu = self._mu
        sig = self._sig
        for k in range(K):
            seq = seqs[k]
            avail = avails[k]
            pdfs = np.empty((M))
            states = states_list[k]
            for t in np.where(avail==False)[0]:
                # calc pdfs for each mixture component
                for m in range(M):
                    pdfs[m] = sp.stats.multivariate_normal.pdf(
                                seq[t], mu[states[t],m], sig[states[t],m],
                                allow_singular=True)
                # select mean of mixture component that gave the maximum pdf
                seq[t] = mu[states[t], np.argmax(pdfs)]
        return seqs
    
    def train_bauwelch_impute_viterbi(self, seqs, rtol, max_iter, avails, 
                                      isRegressive=False):
        """ Train HMM with Baum-Welch by imputing missing observations using 
        Viterbi decoder.
        
        Parameters
        ----------
        seqs : list of float64 2darrays (TxZ)
            training sequences
            Note: len(seqs) = K
        rtol : float64
            relative tolerance (stopping criterion)
        max_iter : float64
            maximum number of Baum-Welch iterations (stopping criterion)
        avails : list of boolean 1darrays (T)
            arrays that indicate whether each element of each sequence is 
            not missing, i.e. True - not missing, False - is missing
        isRegressive : bool, optional
            true: imputation begins from the start of sequence after each imputed gap
            false: imputation performed once
        
        Returns
        -------
        p : float64
            total likelihood of training seqs being produced by the trained HMM
        it : int
            iteration reached during baum-welch training process
        """
        # Choosing the imputation mode:
        if isRegressive:
            raise NotImplementedError, "Regressive is not implemented yet"
        else:
            hmm0 = copy.deepcopy(self)
            hmm0.train_baumwelch(seqs, rtol, max_iter, avails)
            states_decoded = hmm0.decode_viterbi(seqs, avails)
            seqs_imputed = hmm0.impute_by_states(seqs, avails, states_decoded)
            p, it = self.train_baumwelch(seqs_imputed, rtol, max_iter)
        return p, it
        
    def train_bauwelch_impute_mean(self, seqs, rtol, max_iter, avails, params=None):
        """ Train HMM with Baum-Welch by restoring gaps using mean imputation
        
        Parameters
        ----------
        seqs : list of float64 2darrays (TxZ)
            training sequences
            Note: len(seqs) = K
        rtol : float64
            relative tolerance (stopping criterion)
        max_iter : float64
            maximum number of Baum-Welch iterations (stopping criterion)
        avails : list of boolean 1darrays (T)
            arrays that indicate whether each element of each sequence is 
            not missing, i.e. True - not missing, False - is missing
        params : list of one item
            number of neighbours
            
        Returns
        -------
        p : float64
            total likelihood of training seqs being produced by the trained HMM
        it : int
            iteration reached during baum-welch training process
        """
        if params is not None:
            n_neighbours = params[0]
        else:
            n_neighbours=10
        seqs_imp, avails_imp = imp.impute_by_n_neighbours(seqs, avails, n_neighbours,
                                              is_middle=True, method="mean")
        # in case some gaps were not imputed
        seqs_imp = imp.impute_by_whole_seq(seqs_imp, avails_imp, method="mean")
        p, it = self.train_baumwelch(seqs_imp, rtol, max_iter)
        return p, it
        
    def train_baumwelch_gluing(self, seqs, rtol, max_iter, avails):
        """ Glue segments between gaps together and then train Baum-Welch
        
        Parameters
        ----------
        seqs : list of float64 2darrays (TxZ)
            training sequences
            Note: len(seqs) = K
        rtol : float64
            relative tolerance (stopping criterion)
        max_iter : float64
            maximum number of Baum-Welch iterations (stopping criterion)
        avails : list of boolean 1darrays (T)
            arrays that indicate whether each element of each sequence is availiable
        
        Returns
        -------
        p : float64
            total likelihood of training seqs being produced by the trained HMM
        it : int
            iteration reached during baum-welch training process
        """
        K = len(seqs)
        # remove all gaps and just glue remaining segments together
        seqs_glued = []
        for k in range(K):
            glued = seqs[k][avails[k]]  # fancy indexing
            seqs_glued.append(glued)
        p, it = self.train_baumwelch(seqs_glued, rtol, max_iter)
        return p, it

def train_best_hmm_baumwelch(seqs, hmms0_size, N, M, Z, algorithm='marginalization',
                             avails=None, hmms0=None, rtol=1e-1, max_iter=None, 
                             verbose=False):
    """ Train several hmms using baumwelch algorithm and choose the best one
    
    Parameters
    ----------
    seqs : list of float64 2darrays (TxZ)
        list of training sequences
    hmms0_size : int
        number of initial approximations
    N : int
        number of HMM states
    M : int
        number of HMM symbols
    Z : int
        dimensionality of observations
    algorithm : {'marginalization', 'gluing', 'viterbi', 'mean'}
        which algorithm should be used to handle missing observations
    avails : list of boolean 1darrays (T)
            arrays that indicate whether each element of each sequence is 
            not missing (availiable), i.e. True - not missing, False - is missing
    hmms0 : list of GHMMs, optional
        list of initial approximations of HMM parameters
        !note: len(hmms0) == hmms0_size must be fulfilled!
        if not specified, hmms0_size approximations will be generated
    rtol : float64, optional
        relative tolerance (stopping criterion)
    max_iter : float64, optional
        maximum number of Baum-Welch iterations (stopping criterion)
    verbose : bool, optional
        controls whether some debug info should be printed to stdout
    
    Returns
    -------
    hmm_best : GHMM
        best trained hmm or None
    p_max : float64
        likelihood for the best hmm
    iter_best : int
        number of iterations to train the best hmm 
    n_of_best : int
        number of the initial approximation that gave the best hmm
    """
    assert algorithm in ('marginalization', 'gluing', 'viterbi', 'mean'),\
        "Invalid algorithm '{}'".format(algorithm)
    if hmms0 is None:
        mu_est, sig_est = estimate_mu_sig(seqs, N, M, Z, avails)
        hmms = [GHMM(N,M,Z,mu_est,sig_est,seed=np.random.randint(10000))
                    for i in range(hmms0_size-1)]       
        # standard pi, a, tau parameters
        hmms.append(GHMM(N,M,Z,mu_est,sig_est))
    else:
        hmms = copy.deepcopy(hmms0)
    p_max = np.finfo(np.float64).min # minimal value possible
    hmm_best = None
    iter_best = -1 # count number of iters for the best hmm
    n_of_best = -1  # number of the best hmm
    n_of_approx = 0
    # calc and choose the best hmm estimate
    for hmm in hmms:
        if algorithm == 'marginalization':
            p, iteration = hmm.train_baumwelch(seqs, rtol, max_iter, avails)
        if algorithm == 'gluing':
            p, iteration = hmm.train_baumwelch_gluing(seqs, rtol, max_iter, avails)
        if algorithm == 'viterbi':
            p, iteration = hmm.train_bauwelch_impute_viterbi(seqs, rtol, 
                                                             max_iter, avails)
        if algorithm == 'mean':
            p, iteration = hmm.train_bauwelch_impute_mean(seqs, rtol, 
                                                          max_iter, avails)
        if (p_max < p and np.isfinite(p)):
            hmm_best = hmm
            p_max = p
            iter_best = iteration
            n_of_best = n_of_approx
        if verbose:
            print "Baum: n of approximation = " + str(n_of_approx)
            print "p=" + str(p)
            print "iteration = " + str(iteration)
            print hmm._pi
            print hmm._a
            print hmm._tau
            print hmm._mu
            print hmm._sig
            print
        n_of_approx += 1
    return hmm_best, p_max, iter_best, n_of_best

def estimate_mu_sig(seqs, N, M, Z, avails=None):
    """ Estimate values of mu and sig basing on the sequences.
        mu elements are uniformly scattered from min to max seq element
        sig matrixes are diagonal scaled accordingly to min and max seq elements
        
        Parameters
        ----------
        seqs : list of 2darrays (TxZ)
            list of training sequences
        N : int
            number of HMM states
        M : int
            number of distribution mixture components
        Z : int
            dimensionality of observations
        avails : list of boolean 1darrays (T), optional
            arrays that indicate whether each element of each sequence is 
            not missing (availiable), i.e. True - not missing, False - is missing
        
        Returns
        -------
        mu : 3darray (NxMxZ)
            means of normal distributions 
        sig : 4darray (NxMxZxZ)
            covariation matrix of normal distributions
    """
    # TODO: add more clever heuristics to this procedure
    K = len(seqs)
    if avails is None:
        avails = [np.full(shape=seqs[k].shape[0], fill_value=True) for k in range(K)]
    mu = np.empty((N*M,Z))
    sig = np.empty((N,M,Z,Z))
    min_val = np.min([np.min(seqs[k][avails[k]], axis=0) for k in range(K)], axis=0)
    max_val = np.max([np.max(seqs[k][avails[k]], axis=0) for k in range(K)], axis=0)
    step = (max_val - min_val) / (N*M)
    val = min_val + step/2.0
    for i in range(N*M):
        mu[i] += val
        val += step
    mu = np.reshape(mu, newshape=(N,M,Z))
    # TODO: scale sig matrixes
    for i in range(N):
        for j in range(M):
            sig[i,j] = np.eye(Z)
    return mu, sig

def classify_seqs(seqs, hmms):
    """ Label each seq from seqs with number of the hmm which suits seq better
        'suits' i.e. has the biggest likelihood of generating tht sequence
    
    Parameters
    ----------
    seqs : list of 2darrays (TxZ)
        sequences to be classified
    hmms : list of GHMMs 
        list of hmms each of which corresponds to a class
    Returns
    -------
    predictions : list of ints
        list of class labels
    """
    predictions = []
    for k in range(len(seqs)):
        seq = seqs[k]
        p_max = np.finfo(np.float64).min
        label_max = 0
        for label in range(len(hmms)):
            hmm = hmms[label]
            p = hmm.calc_likelihood([seq])
            if p > p_max:
                p_max = p
                label_max = label
        predictions.append(label_max)
    return predictions
    
def estimate_hmm_params_by_seq_and_states(mu, sig, seqs, state_seqs):
    """ to check that sequences agrees with hmm produced it
    
    Parameters
    ----------
    mu : float 3darray (NxMxZ)
        means of normal distributions
    sig : float 4darray (NxMxZ)
        covariation matrix of normal distributions
    seq : float 2darray (TxZ)
        generated sequence
    states : int 1darray (T)
        hidden states appeared during generation
    """
    N, M, Z = mu.shape
    K = len(seqs)
    pi = np.zeros(N)
    a = np.zeros(shape=(N,N))
    tau = np.zeros(shape=(N,M))
    for k in range(K):
        pi_, a_, tau_ = \
            _estimate_hmm_params_by_seq_and_states(mu, sig, seqs[k], state_seqs[k])
        pi += pi_
        a += a_
        tau += tau_
    return pi/K, a/K, tau/K

def _estimate_hmm_params_by_seq_and_states(mu, sig, seq, state_seq):
    N, M, Z = mu.shape
    T = seq.shape[0]
    pi = np.zeros(N)
    a = np.zeros(shape=(N,N))
    tau = np.zeros(shape=(N,M))
    # estimate pi
    scores = np.empty(shape=(N,M))
    for n in range(N):
        for m in range(M):
            scores[n, m] = \
                sp.stats.multivariate_normal.pdf(seq[0], mu[n,m], sig[n,m])
    #state = np.argmax(np.sum(scores, axis=1))
    state = state_seq[0]
    pi[state] = 1.0
    mixture = np.argmax(scores[state])
    tau[state, mixture] += 1.0
    # estimate a
    state_prev = state
    for t in range(1,T):
        for n in range(N):
            for m in range(M):
                scores[n, m] = \
                    sp.stats.multivariate_normal.pdf(seq[t], mu[n,m], sig[n,m])
        #state = np.argmax(np.sum(scores, axis=1))
        state = state_seq[t]
        a[state_prev, state] += 1.0
        mixture = np.argmax(scores[state])
        tau[state, mixture] += 1.0
        state_prev = state
    a = (a.T / np.sum(a, axis=1)).T
    tau = (tau.T / np.sum(tau, axis=1)).T
    return pi, a, tau

def _generate_discrete_distribution(n):
    """ Generate n values > 0.0, whose sum equals 1.0
    """
    xs = np.array(stats.uniform.rvs(size=n))
    return xs / np.sum(xs)
    
def _get_sample_discrete_distr(distr):
    """ Get sample of random value that has discrete distrubution 'distr'
        random values are 0, 1, ...
    """
    val = np.random.uniform()
    cum = 0.0
    for i in range(len(distr)):
        cum += distr[i]
        if val < cum:
            return i
    return distr.size-1

