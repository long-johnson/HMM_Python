# -*- coding: utf-8 -*-

import numpy as np
import sklearn.mixture
import GaussianHMM as ghmm
from itertools import product

sig_val = 0.1
T = 100
K = 100

# hmm1
pi = np.array([0.3, 0.4, 0.3])
a = np.array([[0.1, 0.7, 0.2],
              [0.2, 0.2, 0.6],
              [0.8, 0.1, 0.1]])
tau = np.array([[0.3, 0.4, 0.3],
                [0.3, 0.4, 0.3],
                [0.3, 0.4, 0.3]])
mu = np.array([
              [[0.0, 0.0], [1.0, 1.0], [2.0, 2.0]],
              [[3.0, 3.0], [4.0, 4.0], [5.0, 5.0]],
              [[6.0, 6.0], [7.0, 7.0], [8.0, 8.0]],
              ])
N, M, Z = mu.shape
sig = np.empty((N,M,Z,Z))
for n in range(N):
    for m in range(M):
        sig[n,m,:,:] = np.eye(Z) * sig_val
hmm_orig = ghmm.GHMM(N, M, Z, mu, sig, pi=pi, a=a, tau=tau)


def estimate_mu_sig_gmm(seqs, N, M, avails=None, n_init=5,
                        covariance_type='diag'):
    """ Estimate some HMMs params, namely: means of gaussian mixtures (mu) and
    covariance matrixes of gaussian mixtures given the observation points.
    The estimation is done using gaussian mixture models (GMM).
    This is needed primarily for a good initial guess of HMM parameters.
    
    Parameters
    ----------
    seqs : list (K) of ndarrays (TxZ)
        observation sequences generated by the modelled process
    N, M : int
        probable number of states and mixture components in HMM
    avails : list (K) of boolean 1darrays (T), optional
        missing observations for each sequence
    n_init : int, optional
        Number GMM of initializations to perform. the best result is kept
    covariance_type : {'spherical', 'diag', 'full'}
        Sets the shape and properties of covariance matrix
    Returns
    -------
    mu : 3darray (NxMxZ)
        means of normal distributions 
    sig : 4darray (NxMxZxZ)
        covariation matrix of normal distributions
    """
    assert(covariance_type in ['spherical', 'diag', 'full'])
    points = []
    if avails is None:
        for seq in seqs:
            points += seq.tolist()
    else:
        for seq, avail in zip(seqs, avails):
            points += seq[avail].tolist()
    points = np.array(points)
    gmm = sklearn.mixture.GMM(n_components=N, covariance_type=covariance_type,
                              n_init=n_init)
    pred = gmm.fit_predict(points)
    Z = len(seqs[0][0])
    mu = np.empty((N, M, Z))
    sig = np.empty((N, M, Z, Z))
    for i in range(N):
        local_points = points[pred == i]
        gmm = sklearn.mixture.GMM(n_components=M, covariance_type=covariance_type,
                                  n_init=n_init)
        gmm.fit_predict(local_points)
        print(gmm.means_)
        mu[i] = np.array(gmm.means_)
        if covariance_type == 'full':
            sig[i] = np.array(gmm.covars_)
        else:
            sig_tmp = np.array(gmm.covars_)
            for m in range(N):
                sig[i, m] = np.diag(sig_tmp[m])
    return mu, sig
    
seqs_orig, _ = hmm_orig.generate_sequences(K, T)
avails=[np.full(len(seqs_orig[k]), True, dtype=np.bool) for k in range(K)]
mu, sig = estimate_mu_sig_gmm(
              seqs_orig, N, M, n_init=1, covariance_type='full',
              avails=avails
              )
print("mu")
print(mu)
print("sig")
print(sig)

points = []
for seq, avail in zip(seqs_orig, avails):
    points += seq[avail].tolist()
points = np.array(points)

import matplotlib.pyplot as plt
plt.scatter(points[:, 0], points[:, 1])
plt.show()